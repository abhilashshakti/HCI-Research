	Predicting protein structures with a multiplayer online game -- ⅗

This paper might have path-breaking back in those days (2010), but looks a bit outdated to me for modern times. Just the introduction provides strong signals of deploying “Reinforcement Learning” to the online game. The game of predicting protein structure is not very different from “Alpha Go”, where the computer has to come up with better moves and constrained by the number of possible outcomes (almost infinite!). However, with the advent of deep-learning, which is being applied to problems like these (especially games), and with improved and accessible hardware (high end GPUs), I am curious to see how the same protein prediction game would perform when given the task of learning on its own.

The authors provide lots of results to support their claims that humans perform better (in most cases with substantial margin) than computers on tasks of solving protein puzzles. However, the paper does not elaborate on the design of the computer algorithms being used to compare against human players. Clearly, the Rosetta algorithm is greedy in nature. That’s the reason why it fails to retain partially swapped confirmations, owing to their high energies. This indicates that the Rosetta algorithm is a naive algorithm. Since there is no discussion on the development of Rosetta algorithm, we are constrained not to discuss possible improvements.

The paper starts of with the objective of being accessible to a vast majority of the population all over the world and thus involving them to perform research which would otherwise seem impossible for such population. I was eager in knowing more about the participants of this experiment. Questions such as: What was the age group of most active participants? What was their profession? Which countries had the highest number of participants? kept popping up in my head. Answers to these questions are useful in targeting audience for future experiments and ensuring a better quality of participants. Eg: Chinese are better at solving maths and logical puzzles due to their school’s academic coursework and historical mind games. 

When authors compare human players and computers, they point out that humans outperform computers in most cases, especially the ones which involve non-linear computations. However, humans are handicapped in dealing with structures that donot provide enough visual cues for the next possible rearrangement and the structures which have an unfolded start (far from their optimal solution). Next possible experiment could be to use an “Ensemble” approach where humans and computers solve the puzzle together. The Rosetta algorithm is not a great starter, thus should rely on humans to start the puzzle and then take over the control from time to time. This collaborative effort can possible lead to outcomes which are better than both human and computer experimenting in isolation (at the heart of ensemble models).

Foldit does a great job of assisting novices in figuring out optimal protein folding configurations through a series of introductory levels. The game’s tools and visualizations are especially useful, along with tutorials and help (shown in the diagram but not discussed in the paper). Also, visual cues are helpful for participants not from technical backgrounds to make useful contributions to the study.

----------------
Crowd Research: Open and Scalable University Laboratories -- ⅘

I liked how the authors applied the widely known “Agile Methodology” in the context of research, along with adding scale to each operation being performed. The process of planning, implementing, rectifying and repeating is at the core of modern Scrum practices. Through this paper, I see new possibilities for improving the interaction between higher management and workers in multinational companies which suffer from lack of reach down the hierarchy of the organization. The “skip-level” meetings (as weekly discussions between the PI and crowd researchers in the paper) can be applied in a corporate setting as well. 

The evaluation system being discussed in this paper (especially peer-reviews for credit distribution) has stark similarities to “Blockchains for Cryptocurrency management”. In a decentralized system (such as Crowd Research), the strengths and weaknesses both lie with the distributed stakeholders (thousands of students and professionals participating in Crowd Research along with PI and RAs). This arrangement would ideally strengthen the system but also exposes potentials threats (the possible hacking of credit division as discussed in the paper). The paper calls for a socialist approach to research which would arguably lead to novel outcomes. The paper considers, explores and presents all these possibilities in an unbiased fashion.

The research is evaluated heavily on of getting its contributing students securing admissions in reputed universities such as MIT, Stanford, CMU. We know that publishing papers in top conferences and recommendation letters are just two of the many criteria which universities take into account while evaluating candidate profiles. Also, these universities (like many other universities in US) are ensuring that students from diverse backgrounds (geographically, socially, economically) get admitted to ensure diversity of opinion in the university.So, it is unjustified (of someone getting admitted) to present Crowd research as the sole reason for students getting selected in top universities.

I did not like how the paper presents Crowd Research as a charity work, for improving the lives of underqualified students in developing and underdeveloped nations. When Facebook says “It’s Free and will always be!”, we know that it’s not really free! If course, Facebook is doing a great job of connecting people, but it is not a non-profit organization. Similarly, Crowd Research eventually helps PIs and RAs perform studies which are otherwise impossible to pull off. Also, the paper has no discussion on monetary rewards being associated with conducting research. Were the participants paid for their contributions to the research? The authors have smartly left these questions unanswered, in the name of social service.

This approach of Crowd Research would fail in areas where one needs in-depth domain expertise, something which cannot be imbibed in a few weeks. Also, it is impractical in scenarios which need high computation resources. Research like one conducted by CERN, using Higgs-Boson collider is impossible to be achieved through Crowd Research. Also, the discovery of gravitational waves (by LEGO) looks unlikely through Crowd Research.

