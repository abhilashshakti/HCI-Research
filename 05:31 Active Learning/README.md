	Initially reading this paper appeared to be a daunting task (40 pages long). But, this paper turned out to be one of most simple paper to read and understand. I am (like most of us in this class) passionate about learning, and thus hold opinions and keep myself updated with the latest developments in the education domain. Thus, I found some of the techniques discussed in this paper a bit outdated (like Bayesian models for student performance prediction in knowledge tracing), especially with the recent advancements in online learning (through MOOCs like Coursera). However, I liked how Cognitive Tutor is projected as an assistant to human teachers rather than a replacement to them (unlike most AI assistants, which would fancy their chances of achieving the latter).

One of the things which stood out for me was laying emphasis on the learning trajectory and monitoring it by means of knowledge tracing. I firmly believe that “knowledge gain” is more important than “absolute knowledge” levels. Think of this as the learning curve (or the difference in knowledge, slope of the graph, delta or tan theta) experienced by a student in the course. Therefore, students start at different points and end at different points in knowledge plots. Unfortunately, current pedagogy measures students in absolute terms (mid-term grades and class standings) and not in terms of learning trajectory. I would like to learn about alternate evaluation techniques which take both these methodologies of evaluations into account.

I liked how authors devised an adaptive system which does away with repetitive and lower level tasks as students advance forward in the course by providing templates, graphical hints and spreadsheets. However, there are 2 problems with such techniques: First, reinforcement is an essential component for developing long-term understanding (and memory) of techniques learnt. Thus, it is essential that the students are re-tested on the concepts the system “feels” they have mastered from time to time. Second, hints can often act as a hindrance to the development of core concepts. In the recent years, I have observed that an increasing number of programming courses (offered at university and online) provide students with code skeletons to be filled up (eg: in Jupyter notebooks), thus abstracting the steps required to implement code from scratch. Students are often exposed during programming interviews, when they have to code from scratch and without the help of an IDE, eg: coding on a Google doc.

Cognitive tutors would work pretty well for mathematics and programming courses. I was expecting to see the authors discuss the development of such tools for design (and other creative courses), in future work. My guess is that Cognitive tutor would not work in creative courses, where developing (or inferring) production rules is close to impossible. This is because there are no well-defined right or wrong ways of designing things. Everything is based on heuristics, rather than well-defined theorems (like in Mathematics). But, this should discourage us from researching on the development of digital tutors for design. Rather than customizing the current Cognitive Systems to take up the role of design tutors, we should explore completely new alternatives.

What does ACT-R stand for, and what do the authors mean by "modularity"? Can you think of a personal example of productions rules?
ACT-R (short for "Adaptive Control of Thought-Rational") is a cognitive architecture mainly developed by John Robert Anderson at Carnegie Mellon University. By modularity, authors seek to build knowledge components which can be reused and recombined (like software modules). One of my favorite theories in programming is that of Exception Handling. Production rule for this can be: IF the system throws an exception THEN catch that exception and gracefully exit the program. Eg: handling NullPointerException.

