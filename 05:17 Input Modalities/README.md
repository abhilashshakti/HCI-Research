PixelTone: A Multimodal Interface for Image Editing -- 5/5

I got excited when I read the word “multimodal” in paper’s title. I had heard (and used) this term for the first time when I created a “Multimodal Tutorial on Microwave Remote Sensing” during my internship at Indian Space Research Organization in 2011. At that time, multimodal meant images, text and sound. Voice was still a far-off thing (even to date, voice interfaces not deployed in MOOCs like Coursera and Edx). But, things are changing. We are living in a time when every research and product is garnished with Artificial Intelligence and Machine Learning. We must give credit to the authors for not even using the terms like Artificial Intelligence and Machine Learning, even though PixelTone can serve as a great application of AI (particularly Natural Language Processing (NLP)).

A common problem with voice recognition systems is accuracy. This is understandable because the NLP systems are evolving (especially with advancements in Deep Learning). However, there are ways using which designers can handle the errors more gracefully. The heuristic commonly used as “error recovery” suggests that the users should be guided to next possible steps rather than leaving them to figure things on their own. Early voice assistants displayed error messages like “Sorry, I didn’t catch that. Please try again.” I liked the idea of “gallery mode” which is presented in the paper. This is where the touch interface comes to the rescue of voice interface and allows the user to select one of the tasks that are similar to one he/she expects to perform.

Another aspect of this paper which stood out for me was the way in which the authors presented their contribution to speech recognition, object detection and execution. They’ve made sure that they use speech and computer vision APIs wherever possible and not reinvent the wheel. I had used Google speech APIs for my project in CSE 170 and this helped me focus on the interaction aspects and not dive into the details of NLP. Also, the authors have brilliantly demarcated their system from the existing infrastructure (Fig. 4), something which we should learn while preparing our final reports for this class.

Some students would associate this paper with Prof Klemmer’s idea of “developing a solution for the problem which doesn’t exist.” Do we really need a multimodal interface for a task like image editing? It’s hard to answer this question because most often users don't know what they really want. Users like to be dictators rather being dictated. I firmly believe that voice assistants are here to stay. Slowly but surely they are becoming an integral part of our smart device usage. The problem currently is not so much from the technology point of view, but from usability and user interaction points of view. This is where we (HCI researchers) can take a lead and drive this forward.

Apart from your thoughtful critique, please answer the following question: What does PixelTone do if it does not recognize a command? Why do you think PixelTone has chosen to handle ambiguity in this manner?
When PixelTone does not recognize a command, it maps the unknown words to the set of known operations that PixelTone supports. This is achieved using semantic similarity between terms calculated using shortest path distance. This allows the system to avoid junk responses (explained in the second para above) and provide reasonable responses and allows the system to extend its vocabulary.

