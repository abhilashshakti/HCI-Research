		Ability-Based Design: Concept, Principles and Examples -- 5/5

This is an excellent paper which develops the concept of “Ability-Based Design”. The authors focus on the novel idea of focussing on the abilities and not the disabilities of the users while developing interfaces. Now when I look back, I realize that my approach for making my design “inclusive” was always flawed. I had always focussed on things that could make my design universal by means of add-ons, thus targeting the weaknesses of my end-users rather than their strengths. Also, I especially liked the 7 guiding principles put forward in this paper (Table - 1), which I uphold in line with Nielsen's 10 heuristics for Interaction Design. After reading this paper, it’s only fair to conclude that software is a powerful enabler (probably more than hardware) to make UIs adaptive in real-world scenarios.

When the world is discussing the recent demo of Google Duplex (in this year’s Google I/O), there’s an important development which most of us have missed. That is the development of “Morse code input to GBoard” (https://techcrunch.com/2018/05/08/google-adds-morse-code-input-to-gboard/). With this new feature, Google has allowed it’s differently-abled users to type Morse code (through long and short signals) using the same soft-input keyboard as millions of users all around the world use!
Clearly, we have come a long way from the time of Word+, the scanner app which gained popularity after being adopted by Stephen Hawkings back in 1985.

I am glad that authors talk about “context-sensitive user interfaces” (under Extraordinary Human-Computer Interaction). They throw light on situations where any normal person can face difficulties (or disabilities) in unusual environments. I have been following (and worked on) user-interfaces for improving the driving experience. Applications such as Google Auto, Apple Carplay and Samsung Carmode all focus on distraction-free driving without a significant loss in smart device user experience. An important thing which I noticed while developing features for Samsung Carmode was that it is sometimes beneficial to actually restrict features rather than adding them to make interfaces adaptive.eg: when in “parked mode”, the app can allow the user to send text messages through both touch (typing) and voice interfaces. While in “driving mode”, the touch interface should be disabled and sending text messages should only be supported through the voice interface. This significantly reduces driver distraction and improves safety.

The authors do a great job in presenting the facts in tabular form, with screenshots and detailed explanations in most of the sections of this paper. I think Section 3 (Prior approaches to accessible computing) could have been presented in a better way. There are 9 approaches which have been discussed and there’s a lot of overlap between these approaches. While reading this section, I sometimes had the feeling that even authors were not confident about the exact nuances and differences between these approaches. A better way could have been to use Venn-diagrams to show the overlap between these and a table of comparison to depict the differences between these techniques. Such depictions would have made my understanding clearer.

Apart from your thoughtful critique, please answer the following question: Figure 6 shows three different variations of an interface. How does SUPPLE decide on which one to show?
Supple first tests the users on pointing, dragging and list selection tasks. From these tasks, Supple builds a predictive model of a user’s movement time that is used to parameterize a search for the “optimal” interface. Widget selection, size, position and grouping are all part of the search space. Once the optimal interface is found, this becomes the “global interface” and the subsequent interaction happens with this newly adapted user interface.

