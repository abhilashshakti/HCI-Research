	The Surprising Power of Online Experiments -- ⅘

This is a fun article to read. I especially like the “Case Study” format of this article. No wonder this is published in Harvard Business Review, this reading is tailor-made for B schools. Although, Ron is associated with Microsoft’s Experimentation Team, he does a great job in being unbiased while putting his views forward. Both the successes (change in color experiment), as well as failures (integrating Bing with social media), are discussed in detail. Also, the raw sense of humor, in places like “companies need to kiss a lot of frogs to find a prince” keeps the readers engaged!

I agree with Ron that the value of running experiments (and prototyping) is undervalued. This makes it hard to convince the program managers to invest in such process. I can draw a similar experience I faced when I was working for Samsung Research, India. I had proposed the idea of adding a “buffering animation” to the existing “play/pause” button in the Carmode application. Initially, the manager resisted because it required some time and effort from the design team (we had a Centralized Design model at Samsung). So, I went ahead and prototyped this idea. To convince the manager and design team, I created a short video and a presentation of the comparison between the two designs (something which design team should have done). Also, I added a slide on how this could be a simple yet differentiating feature from Samsung’s competitors (Google and Apple). The prototype was accepted and later incorporated as a feature in the Carmode application.

I wish the authors had discussed the data privacy of the participants in online experiments. This idea has gained importance with the recent events. A/B tests are conducted to collect data and visualize them in different aspects (geography, platform etc.) If there are just a handful of participants, anonymity may be lost. eg: if there’s just one participant from California, it may not be too difficult to find who that was. So, I feel user consent is important even in massive online experiments. This process can have positive consequences like:
Online experiments often lead to confusion among regular users (Youtube keeps changing the location and language buttons to top and bottom of the screen, which is an annoying experience if you don’t know about A/B testing). Making user informed about A/B test may clear the cloud of uncertainty and prevent loss of trust.


I don’t completely agree with authors when they say “If you thought something was going to happen and it didn’t, then you haven’t learned much”. I believe failures teach us as much (and sometimes more) than successes. Most of the prime features which we see today in our favorite applications and websites are resultant of failed attempts at finding the breakthrough idea. Also, failing of an experiment has a cascading effect. Think about it this way: If I know that a parent node of a tree (of possible experiments) has failed, I can safely avoid running the child experiments. So, one failed experiment has helped me save time on running an entire branch of experiments.

Pick any one of the other three sources and detail one way in which it supports and one way it refutes the arguments made in this article.
In the article “Goodbye Google/Hello Twitter”, Dough puts forward the thought of how a product means different things for different users and thus used in so many different ways. This aligns with the idea of advanced and novice users as discussed in this paper. For each Microsoft product, there exists a set of advanced users who care more about new and advanced features coming in the market every few days and then there are novice users who want the essential components to work perfectly and rarely indulge into something new. What Dough does not agree with is the idea of “Data-driven decision making”. Whereas Ron is a firm supporter of backing every decision with data (collected from web experiments) and not simply relying on one's intuition.

